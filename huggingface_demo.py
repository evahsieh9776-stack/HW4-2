"""
Hugging Face Inference API ç¤ºä¾‹
ä½¿ç”¨æ–¹æ¡ˆäºŒ:é€šè¿‡ Hugging Face Inference API è°ƒç”¨æ¨¡å‹
æ— éœ€ä¸‹è½½æ¨¡å‹,ç›´æ¥ä½¿ç”¨äº‘ç«¯ API
"""

# ============================================
# ç¬¬ä¸€æ­¥:å®‰è£…å¿…è¦çš„åº“
# ============================================
# åœ¨ Jupyter Notebook ä¸­è¿è¡Œ:
# !pip install huggingface_hub

# ============================================
# ç¬¬äºŒæ­¥:å¯¼å…¥åº“å¹¶è®¾ç½® API Token
# ============================================
from huggingface_hub import InferenceClient
import os

# å¦‚æœåœ¨ Google Colab ä¸­ä½¿ç”¨:
# from google.colab import userdata
# hf_token = userdata.get('HuggingFace')  # ä» Colab Secrets è¯»å–

# å¦‚æœåœ¨æœ¬åœ° Jupyter ä½¿ç”¨,ç›´æ¥è®¾ç½®:
# hf_token = "your_huggingface_token_here"  # æ›¿æ¢ä¸ºä½ çš„ token

# æˆ–è€…ä»ç¯å¢ƒå˜é‡è¯»å–:
# hf_token = os.getenv('HF_TOKEN')

# ============================================
# ç¬¬ä¸‰æ­¥:åˆ›å»º Inference Client
# ============================================
def create_hf_client(token=None):
    """
    åˆ›å»º Hugging Face Inference Client
    
    å‚æ•°:
        token: Hugging Face API token (å¯é€‰,æŸäº›å…¬å¼€æ¨¡å‹ä¸éœ€è¦)
    
    è¿”å›:
        InferenceClient å®ä¾‹
    """
    if token:
        return InferenceClient(token=token)
    else:
        # ä¸ä½¿ç”¨ token,åªèƒ½è®¿é—®å…¬å¼€æ¨¡å‹
        return InferenceClient()


# ============================================
# ç¬¬å››æ­¥:å®šä¹‰ä¸åŒçš„ä½¿ç”¨ç¤ºä¾‹
# ============================================

def example_1_text_generation(client, prompt="ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"):
    """
    ç¤ºä¾‹ 1: åŸºç¡€æ–‡æœ¬ç”Ÿæˆ
    """
    print("=" * 50)
    print("ç¤ºä¾‹ 1: åŸºç¡€æ–‡æœ¬ç”Ÿæˆ")
    print("=" * 50)
    
    response = client.text_generation(
        prompt=prompt,
        model="Qwen/Qwen2.5-1.5B-Instruct",  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹
        max_new_tokens=200,
        temperature=0.7
    )
    
    print(f"è¾“å…¥: {prompt}")
    print(f"è¾“å‡º: {response}")
    print()
    return response


def example_2_chat_completion(client, user_message="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?"):
    """
    ç¤ºä¾‹ 2: å¯¹è¯è¡¥å…¨ (æ¨èä½¿ç”¨)
    """
    print("=" * 50)
    print("ç¤ºä¾‹ 2: å¯¹è¯è¡¥å…¨")
    print("=" * 50)
    
    messages = [
        {
            "role": "system", 
            "content": "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„ AI åŠ©æ‰‹,è¯·ç”¨ç¹ä½“ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
        },
        {
            "role": "user", 
            "content": user_message
        }
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",  # ä¸­æ–‡èƒ½åŠ›å¼ºçš„æ¨¡å‹
        max_tokens=500,
        temperature=0.7
    )
    
    print(f"ç”¨æˆ·: {user_message}")
    print(f"AI: {response.choices[0].message.content}")
    print()
    return response.choices[0].message.content


def example_3_lucky_vicky(client, event="ä»Šå¤©å’–å•¡ç‘åˆ°é›»è…¦ä¸Šäº†!"):
    """
    ç¤ºä¾‹ 3: å‘˜ç‘›å¼æ€è€ƒç”Ÿæˆå™¨ (Lucky Vicky)
    """
    print("=" * 50)
    print("ç¤ºä¾‹ 3: å“¡ç‘›å¼æ€è€ƒç”Ÿæˆå™¨")
    print("=" * 50)
    
    system_prompt = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": event}
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=500,
        temperature=0.8  # ç¨é«˜çš„æ¸©åº¦è®©å›å¤æ›´æœ‰åˆ›æ„
    )
    
    lucky_post = response.choices[0].message.content
    
    print(f"äº‹ä»¶: {event}")
    print(f"\nğŸ“£ å“¡ç‘›å¼è²¼æ–‡:\n{lucky_post}")
    print()
    return lucky_post


def example_4_sentiment_analysis(client, text="ä»Šå¤©å¤©æ°£çœŸå¥½!"):
    """
    ç¤ºä¾‹ 4: æƒ…æ„Ÿåˆ†æ
    """
    print("=" * 50)
    print("ç¤ºä¾‹ 4: æƒ…æ„Ÿåˆ†æ")
    print("=" * 50)
    
    # ä½¿ç”¨å¯¹è¯æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æƒ…æ„Ÿ,åªå›ç­”:æ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§ã€‚"
        },
        {
            "role": "user",
            "content": f"è¯·åˆ†æè¿™æ®µæ–‡å­—çš„æƒ…æ„Ÿ: {text}"
        }
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-1.5B-Instruct",
        max_tokens=50
    )
    
    sentiment = response.choices[0].message.content
    
    print(f"æ–‡æœ¬: {text}")
    print(f"æƒ…æ„Ÿ: {sentiment}")
    print()
    return sentiment


def example_5_translation(client, text="Hello, how are you?", target_lang="ç¹é«”ä¸­æ–‡"):
    """
    ç¤ºä¾‹ 5: ç¿»è¯‘
    """
    print("=" * 50)
    print("ç¤ºä¾‹ 5: ç¿»è¯‘")
    print("=" * 50)
    
    messages = [
        {
            "role": "system",
            "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}ã€‚"
        },
        {
            "role": "user",
            "content": text
        }
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=200
    )
    
    translation = response.choices[0].message.content
    
    print(f"åŸæ–‡: {text}")
    print(f"è¯‘æ–‡: {translation}")
    print()
    return translation


def example_6_streaming_response(client, prompt="è«‹è¬›ä¸€å€‹æœ‰è¶£çš„æ•…äº‹"):
    """
    ç¤ºä¾‹ 6: æµå¼å“åº” (é€å­—è¾“å‡º)
    """
    print("=" * 50)
    print("ç¤ºä¾‹ 6: æµå¼å“åº”")
    print("=" * 50)
    print(f"æç¤º: {prompt}\nå›å¤: ", end="")
    
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    full_response = ""
    
    for token in client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-1.5B-Instruct",
        max_tokens=300,
        stream=True
    ):
        chunk = token.choices[0].delta.content
        print(chunk, end="", flush=True)
        full_response += chunk
    
    print("\n")
    return full_response


# ============================================
# ç¬¬äº”æ­¥:ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
# ============================================
def run_all_examples(token=None):
    """
    è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    
    å‚æ•°:
        token: Hugging Face API token (å¯é€‰)
    """
    print("\nğŸš€ å¼€å§‹è¿è¡Œ Hugging Face Inference API ç¤ºä¾‹\n")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = create_hf_client(token)
    
    try:
        # ç¤ºä¾‹ 1: åŸºç¡€æ–‡æœ¬ç”Ÿæˆ
        example_1_text_generation(client, "å°ç£æœ€æœ‰åçš„å°åƒæ˜¯ä»€éº¼?")
        
        # ç¤ºä¾‹ 2: å¯¹è¯è¡¥å…¨
        example_2_chat_completion(client, "è«‹æ¨è–¦ä¸‰å€‹å°åŒ—çš„æ—…éŠæ™¯é»")
        
        # ç¤ºä¾‹ 3: Lucky Vicky ç”Ÿæˆå™¨
        example_3_lucky_vicky(client, "ä»Šå¤©å‡ºé–€å¿˜è¨˜å¸¶å‚˜,çµæœä¸‹å¤§é›¨")
        
        # ç¤ºä¾‹ 4: æƒ…æ„Ÿåˆ†æ
        example_4_sentiment_analysis(client, "é€™å€‹ç”¢å“çœŸçš„å¤ªæ£’äº†!")
        
        # ç¤ºä¾‹ 5: ç¿»è¯‘
        example_5_translation(client, "Machine learning is amazing!", "ç¹é«”ä¸­æ–‡")
        
        # ç¤ºä¾‹ 6: æµå¼å“åº”
        # example_6_streaming_response(client, "è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹äººå·¥æ™ºæ…§")
        
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\nğŸ’¡ æç¤º:")
        print("1. ç¡®ä¿å·²å®‰è£… huggingface_hub: pip install huggingface_hub")
        print("2. æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦ API token")
        print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")


# ============================================
# ä½¿ç”¨è¯´æ˜
# ============================================
"""
åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨:

# æ–¹æ³• 1: ä¸ä½¿ç”¨ token (ä»…å…¬å¼€æ¨¡å‹)
from huggingface_hub import InferenceClient
client = InferenceClient()

# æ–¹æ³• 2: ä½¿ç”¨ token (æ¨è)
from huggingface_hub import InferenceClient
client = InferenceClient(token="your_token_here")

# è¿è¡Œå•ä¸ªç¤ºä¾‹
example_3_lucky_vicky(client, "ä»Šå¤©é²åˆ°äº†10åˆ†é˜")

# æˆ–è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
run_all_examples(token="your_token_here")
"""


# ============================================
# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
# ============================================
if __name__ == "__main__":
    print("è¯·åœ¨ Jupyter Notebook ä¸­å¯¼å…¥å¹¶ä½¿ç”¨æ­¤æ¨¡å—")
    print("\nç¤ºä¾‹ä»£ç :")
    print("from huggingface_demo import *")
    print("client = create_hf_client()")
    print("example_3_lucky_vicky(client, 'ä»Šå¤©å’–å•¡ç‘äº†')")
