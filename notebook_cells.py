"""
ğŸ¤— Hugging Face Inference API - Notebook Cell ä»£ç 
ç›´æ¥å¤åˆ¶åˆ° Jupyter Notebook çš„ Cell ä¸­è¿è¡Œ
"""

# ============================================
# Cell 1: å®‰è£…åº“
# ============================================
!pip install huggingface_hub


# ============================================
# Cell 2: å¯¼å…¥å¹¶è®¾ç½®
# ============================================
from huggingface_hub import InferenceClient
from google.colab import userdata
import os

# ä» Colab Secrets è¯»å– token
# è¯·å…ˆåœ¨å·¦ä¾§ ğŸ”‘ å›¾æ ‡æ·»åŠ åä¸º 'HuggingFace' çš„ Secret
hf_token = userdata.get('HuggingFace')

# åˆ›å»ºå®¢æˆ·ç«¯
client = InferenceClient(token=hf_token)

print("âœ… Hugging Face å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ!")


# ============================================
# Cell 3: ç®€å•æµ‹è¯•
# ============================================
# æµ‹è¯•åŸºç¡€å¯¹è¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„ AI åŠ©æ‰‹,è¯·ç”¨ç¹ä½“ä¸­æ–‡å›ç­”ã€‚"},
    {"role": "user", "content": "ä½ å¥½!è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
]

response = client.chat_completion(
    messages=messages,
    model="Qwen/Qwen2.5-7B-Instruct",
    max_tokens=200
)

print(response.choices[0].message.content)


# ============================================
# Cell 4: Lucky Vicky å‡½æ•° (Hugging Face ç‰ˆæœ¬)
# ============================================
def lucky_post_hf(event):
    """
    ä½¿ç”¨ Hugging Face çš„å“¡ç‘›å¼æ€è€ƒç”Ÿæˆå™¨
    """
    system_prompt = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": event}
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=500,
        temperature=0.8
    )
    
    return response.choices[0].message.content


# ============================================
# Cell 5: æµ‹è¯• Lucky Vicky
# ============================================
# æµ‹è¯•å‡ ä¸ªä¾‹å­
test_events = [
    "ä»Šå¤©å’–å•¡ç‘åˆ°é›»è…¦ä¸Šäº†!",
    "å‡ºé–€å¿˜è¨˜å¸¶å‚˜,çµæœä¸‹å¤§é›¨",
    "è€ƒè©¦è€ƒå¾—ä¸å¤ªå¥½"
]

for event in test_events:
    print(f"\n{'='*60}")
    print(f"äº‹ä»¶: {event}")
    print(f"{'='*60}")
    result = lucky_post_hf(event)
    print(f"\nğŸ“£ å“¡ç‘›å¼è²¼æ–‡:\n{result}\n")


# ============================================
# Cell 6: å¤šæä¾›å•†ç‰ˆæœ¬ (æ•´åˆ Groq å’Œ Hugging Face)
# ============================================
def lucky_post_multi(prompt, provider="huggingface"):
    """
    æ”¯æŒå¤šä¸ª AI æä¾›å•†çš„ Lucky Vicky
    
    å‚æ•°:
        prompt: äº‹ä»¶æè¿°
        provider: "huggingface" æˆ– "groq"
    """
    system = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""
    
    if provider.lower() == "huggingface":
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]
        response = client.chat_completion(
            messages=messages,
            model="Qwen/Qwen2.5-7B-Instruct",
            max_tokens=500,
            temperature=0.8
        )
        return response.choices[0].message.content
    
    elif provider.lower() == "groq":
        # ä½¿ç”¨åŸæœ‰çš„ reply å‡½æ•°
        return reply(system=system, prompt=prompt, 
                    provider="groq", model="openai/gpt-oss-120b")
    
    else:
        return "âŒ ä¸æ”¯æŒçš„æä¾›å•†,è¯·é€‰æ‹© 'huggingface' æˆ– 'groq'"


# æµ‹è¯•ä¸¤ä¸ªæä¾›å•†
event = "ä»Šå¤©é²åˆ°äº†10åˆ†é˜"

print("ä½¿ç”¨ Hugging Face:")
print(lucky_post_multi(event, "huggingface"))

print("\n" + "="*60 + "\n")

print("ä½¿ç”¨ Groq:")
print(lucky_post_multi(event, "groq"))


# ============================================
# Cell 7: Gradio App (æ•´åˆç‰ˆ)
# ============================================
import gradio as gr

# ç³»ç»Ÿæç¤º
system = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""

def lucky_vicky_app(prompt, provider_choice):
    """Gradio åº”ç”¨çš„ä¸»å‡½æ•°"""
    
    if provider_choice == "ğŸ¤— Hugging Face":
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]
        response = client.chat_completion(
            messages=messages,
            model="Qwen/Qwen2.5-7B-Instruct",
            max_tokens=500,
            temperature=0.8
        )
        return response.choices[0].message.content
    
    elif provider_choice == "âš¡ Groq":
        return reply(system=system, prompt=prompt, 
                    provider="groq", model="openai/gpt-oss-120b")
    
    else:
        return "è¯·é€‰æ‹©ä¸€ä¸ª AI æä¾›å•†"

# åˆ›å»ºç•Œé¢
with gr.Blocks(
    title="å“¡ç‘›å¼æ€è€ƒç”¢ç”Ÿå™¨ - å¤šæ¨¡å‹ç‰ˆ",
    theme=gr.themes.Soft()
) as demo:
    
    # æ ‡é¢˜
    gr.Markdown("""
    # ê’°*ËŠáµ•Ë‹ê’± å“¡ç‘›å¼æ€è€ƒç”¢ç”Ÿå™¨ Lucky Vicky ğŸŒˆ
    
    è«‹è¼¸å…¥ä¸€ä»¶ä½ è¦ºå¾—è¶…å°äº‹ï¼Œç”šè‡³æœ‰é»å€’æ¥£çš„äº‹ï¼Œ
    è®“æˆ‘å¹«ä½ ç”¨å“¡ç‘›å¼æ€è€ƒï¼Œè¶…æ­£å‘çš„æ–¹å¼é‡æ–°è©®é‡‹ï¼
    
    **ç¾åœ¨æ”¯æŒå¤šå€‹ AI æ¨¡å‹!** ğŸš€
    """)
    
    with gr.Row():
        with gr.Column(scale=2):
            # è¾“å…¥æ¡†
            user_input = gr.Textbox(
                label="ğŸ“ ä»Šå¤©ç™¼ç”Ÿçš„äº‹æƒ…æ˜¯â€¦", 
                placeholder="ä¾‹å¦‚:ä»Šå¤©å‡ºé–€å°±ä¸‹å¤§é›¨, å¯æ˜¯å¿˜äº†å¸¶å‚˜...",
                lines=4
            )
        
        with gr.Column(scale=1):
            # æ¨¡å‹é€‰æ‹©
            provider_dropdown = gr.Dropdown(
                choices=["ğŸ¤— Hugging Face", "âš¡ Groq"],
                value="ğŸ¤— Hugging Face",
                label="ğŸ¤– é¸æ“‡ AI æ¨¡å‹"
            )
            
            # æäº¤æŒ‰é’®
            submit_btn = gr.Button(
                "âœ¨ Lucky Vicky é­”æ³•!", 
                variant="primary",
                size="lg"
            )
    
    # è¾“å‡ºæ¡†
    output = gr.Textbox(
        label="ğŸ“£ å“¡ç‘›å¼è²¼æ–‡", 
        lines=12,
        show_copy_button=True
    )
    
    # ç¤ºä¾‹
    gr.Examples(
        examples=[
            ["ä»Šå¤©å’–å•¡ç‘åˆ°é›»è…¦ä¸Šäº†!", "ğŸ¤— Hugging Face"],
            ["å‡ºé–€å¿˜è¨˜å¸¶å‚˜,çµæœä¸‹å¤§é›¨", "ğŸ¤— Hugging Face"],
            ["è€ƒè©¦è€ƒå¾—ä¸å¤ªå¥½", "âš¡ Groq"],
            ["ä»Šå¤©é²åˆ°äº†10åˆ†é˜", "ğŸ¤— Hugging Face"],
            ["æ‰‹æ©Ÿæ‰åˆ°æ°´è£¡äº†", "âš¡ Groq"],
        ],
        inputs=[user_input, provider_dropdown]
    )
    
    # ç»‘å®šäº‹ä»¶
    submit_btn.click(
        fn=lucky_vicky_app, 
        inputs=[user_input, provider_dropdown], 
        outputs=output
    )
    
    # é¡µè„š
    gr.Markdown("""
    ---
    ğŸ’¡ **æç¤º**: 
    - Hugging Face ä½¿ç”¨ Qwen 2.5 æ¨¡å‹,ä¸­æ–‡èƒ½åŠ›å¼º
    - Groq é€Ÿåº¦æ›´å¿«,ä½¿ç”¨ GPT-OSS æ¨¡å‹
    - å¯ä»¥å°è¯•ä¸åŒæ¨¡å‹,çœ‹çœ‹å“ªä¸ªæ›´ç¬¦åˆä½ çš„æœŸå¾…!
    """)

# å¯åŠ¨åº”ç”¨
demo.launch(share=True, debug=True)


# ============================================
# Cell 8: å…¶ä»–å®ç”¨åŠŸèƒ½
# ============================================

# åŠŸèƒ½ 1: æƒ…æ„Ÿåˆ†æ
def hf_sentiment(text):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯æƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚åˆ†ææ–‡æœ¬æƒ…æ„Ÿ,åªå›ç­”:æ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§ã€‚"},
        {"role": "user", "content": f"åˆ†ææƒ…æ„Ÿ: {text}"}
    ]
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-1.5B-Instruct",
        max_tokens=20
    )
    return response.choices[0].message.content

# åŠŸèƒ½ 2: ç¿»è¯‘
def hf_translate(text, target_lang="ç¹é«”ä¸­æ–‡"):
    messages = [
        {"role": "system", "content": f"ä½ æ˜¯ä¸“ä¸šç¿»è¯‘ã€‚å°†æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}ã€‚"},
        {"role": "user", "content": text}
    ]
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=200
    )
    return response.choices[0].message.content

# åŠŸèƒ½ 3: æ‘˜è¦ç”Ÿæˆ
def hf_summarize(text):
    messages = [
        {"role": "system", "content": "è¯·ç”¨ç¹ä½“ä¸­æ–‡æ€»ç»“ä»¥ä¸‹å†…å®¹,ä¿æŒç®€æ´ã€‚"},
        {"role": "user", "content": text}
    ]
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=300
    )
    return response.choices[0].message.content

# æµ‹è¯•è¿™äº›åŠŸèƒ½
print("æƒ…æ„Ÿåˆ†æ:", hf_sentiment("é€™å€‹ç”¢å“çœŸçš„å¤ªæ£’äº†!"))
print("\nç¿»è¯‘:", hf_translate("Hello, how are you today?"))
print("\næ‘˜è¦:", hf_summarize("äººå·¥æ™ºæ…§æ˜¯ä¸€é–€ç ”ç©¶å¦‚ä½•è®“æ©Ÿå™¨æ¨¡æ“¬äººé¡æ™ºæ…§çš„å­¸ç§‘..."))
