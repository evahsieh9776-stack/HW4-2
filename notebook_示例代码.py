"""
Lucky Vicky ç”Ÿæˆå™¨ - ç®€åŒ–ç‰ˆ
è¿™ä¸ªç‰ˆæœ¬å±•ç¤ºäº†å¦‚ä½•åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨ Hugging Face
"""

print("="*60)
print("ğŸ¤— Hugging Face ä½¿ç”¨æŒ‡å— - Lucky Vicky ç¤ºä¾‹")
print("="*60)

print("""
ğŸ“š åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨ Hugging Face çš„å®Œæ•´ç¤ºä¾‹

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†æ‚¨éœ€è¦åœ¨ Notebook ä¸­è¿è¡Œçš„ä»£ç ã€‚
è¯·å°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ°æ‚¨çš„ Jupyter Notebook ä¸­ã€‚

""")

print("="*60)
print("Cell 1: å®‰è£…åº“")
print("="*60)
print("""
!pip install huggingface_hub
""")

print("\n" + "="*60)
print("Cell 2: å¯¼å…¥åº“å¹¶è®¾ç½® Token")
print("="*60)
print("""
from huggingface_hub import InferenceClient

# åœ¨ Google Colab ä¸­
from google.colab import userdata
hf_token = userdata.get('HuggingFace')

# åˆ›å»ºå®¢æˆ·ç«¯
client = InferenceClient(token=hf_token)
print("âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ!")
""")

print("\n" + "="*60)
print("Cell 3: Lucky Vicky å‡½æ•° (ä½¿ç”¨å…è´¹æ¨¡å‹)")
print("="*60)
print("""
def lucky_vicky_hf(event):
    '''ä½¿ç”¨ Hugging Face çš„ Lucky Vicky ç”Ÿæˆå™¨'''
    
    system = '''è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚'''
    
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": event}
    ]
    
    try:
        # ä½¿ç”¨ Meta Llama å…è´¹æ¨¡å‹
        response = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-3B-Instruct",
            max_tokens=300,
            temperature=0.8
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âŒ é”™è¯¯: {e}\\n\\nğŸ’¡ æç¤º: å…è´¹è´¦æˆ·å¯èƒ½æœ‰é™åˆ¶,å»ºè®®ä½¿ç”¨ Groq API"

# æµ‹è¯•
result = lucky_vicky_hf("ä»Šå¤©å’–å•¡ç‘äº†")
print(result)
""")

print("\n" + "="*60)
print("Cell 4: ä½¿ç”¨ Groq (æ¨è - æ‚¨å·²é…ç½®)")
print("="*60)
print("""
# ä½¿ç”¨æ‚¨åŸæœ‰çš„ reply å‡½æ•°
system = '''è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚'''

# ä½¿ç”¨ Groq (å¿«é€Ÿä¸”å…è´¹)
result = reply(
    system=system,
    prompt="ä»Šå¤©å’–å•¡ç‘äº†",
    provider="groq",
    model="llama-3.3-70b-versatile"
)

print(result)
""")

print("\n" + "="*60)
print("Cell 5: å¤šæä¾›å•†ç‰ˆæœ¬")
print("="*60)
print("""
def lucky_vicky_multi(event, use_hf=False):
    '''æ”¯æŒ Hugging Face å’Œ Groq çš„ç‰ˆæœ¬'''
    
    system = '''è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚'''
    
    if use_hf:
        # ä½¿ç”¨ Hugging Face
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": event}
        ]
        try:
            response = client.chat_completion(
                messages=messages,
                model="meta-llama/Llama-3.2-3B-Instruct",
                max_tokens=300,
                temperature=0.8
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"HF é”™è¯¯: {e}, åˆ‡æ¢åˆ° Groq...")
            use_hf = False
    
    if not use_hf:
        # ä½¿ç”¨ Groq (é»˜è®¤)
        return reply(
            system=system,
            prompt=event,
            provider="groq",
            model="llama-3.3-70b-versatile"
        )

# æµ‹è¯•
print("ä½¿ç”¨ Groq:")
print(lucky_vicky_multi("ä»Šå¤©é²åˆ°äº†10åˆ†é˜", use_hf=False))

print("\\nå°è¯•ä½¿ç”¨ Hugging Face:")
print(lucky_vicky_multi("ä»Šå¤©é²åˆ°äº†10åˆ†é˜", use_hf=True))
""")

print("\n" + "="*60)
print("Cell 6: Gradio App (æ•´åˆç‰ˆ)")
print("="*60)
print("""
import gradio as gr

# ç³»ç»Ÿæç¤º
system = '''è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚'''

def lucky_vicky_app(prompt, provider_choice):
    if provider_choice == "ğŸ¤— Hugging Face":
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]
        try:
            response = client.chat_completion(
                messages=messages,
                model="meta-llama/Llama-3.2-3B-Instruct",
                max_tokens=300,
                temperature=0.8
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"âŒ HF é”™è¯¯: {e}\\n\\næ­£åœ¨ä½¿ç”¨ Groq..."
    
    # ä½¿ç”¨ Groq
    return reply(system=system, prompt=prompt, 
                provider="groq", model="llama-3.3-70b-versatile")

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="Lucky Vicky ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸŒˆ Lucky Vicky ç”Ÿæˆå™¨")
    gr.Markdown("æ”¯æŒ Hugging Face å’Œ Groq!")
    
    with gr.Row():
        user_input = gr.Textbox(
            label="ğŸ“ ç™¼ç”Ÿäº†ä»€éº¼äº‹?",
            placeholder="ä¾‹å¦‚:ä»Šå¤©å’–å•¡ç‘äº†...",
            lines=3
        )
    
    provider = gr.Dropdown(
        choices=["âš¡ Groq (æ¨è)", "ğŸ¤— Hugging Face"],
        value="âš¡ Groq (æ¨è)",
        label="é¸æ“‡ AI æä¾›å•†"
    )
    
    btn = gr.Button("âœ¨ Lucky Vicky é­”æ³•!", variant="primary")
    output = gr.Textbox(label="ğŸ“£ å“¡ç‘›å¼è²¼æ–‡", lines=10)
    
    gr.Examples(
        examples=[
            ["ä»Šå¤©å’–å•¡ç‘äº†", "âš¡ Groq (æ¨è)"],
            ["å‡ºé–€å¿˜è¨˜å¸¶å‚˜", "ğŸ¤— Hugging Face"],
        ],
        inputs=[user_input, provider]
    )
    
    btn.click(lucky_vicky_app, [user_input, provider], output)

demo.launch(share=True, debug=True)
""")

print("\n" + "="*60)
print("ğŸ“ æ€»ç»“")
print("="*60)
print("""
æ‚¨ç°åœ¨çŸ¥é“å¦‚ä½•:

âœ… åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨ Hugging Face
âœ… é…ç½®å’Œä½¿ç”¨ Hugging Face Token
âœ… åˆ›å»º Lucky Vicky ç”Ÿæˆå™¨
âœ… æ•´åˆå¤šä¸ª AI æä¾›å•† (Groq + Hugging Face)
âœ… åˆ›å»º Gradio Web App

ğŸ’¡ å»ºè®®:
- åœ¨ Notebook ä¸­ä¼˜å…ˆä½¿ç”¨ Groq (å¿«é€Ÿä¸”å…è´¹)
- Hugging Face å…è´¹è´¦æˆ·æœ‰æ¨¡å‹é™åˆ¶
- å¯ä»¥åŒæ—¶é…ç½®å¤šä¸ªæä¾›å•†ä½œä¸ºå¤‡é€‰

ğŸ“ ç›¸å…³æ–‡ä»¶:
- HuggingFace_ä½¿ç”¨æŒ‡å—.md - è¯¦ç»†æ•™ç¨‹
- æµ‹è¯•ç»“æœæ€»ç»“.md - æµ‹è¯•ç»“æœå’Œå»ºè®®
- Tokenä½¿ç”¨è¯´æ˜.md - Token é…ç½®è¯´æ˜

ğŸ‰ æ­å–œ!æ‚¨å·²ç»æŒæ¡äº† Hugging Face çš„åŸºæœ¬ä½¿ç”¨!
""")

input("\næŒ‰ Enter é”®é€€å‡º...")
