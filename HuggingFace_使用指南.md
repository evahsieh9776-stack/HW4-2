# ğŸ¤— Hugging Face Inference API ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [å®‰è£…ä¸è®¾ç½®](#å®‰è£…ä¸è®¾ç½®)
2. [è·å– API Token](#è·å–-api-token)
3. [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
4. [Lucky Vicky é›†æˆ](#lucky-vicky-é›†æˆ)
5. [å®Œæ•´ç¤ºä¾‹ä»£ç ](#å®Œæ•´ç¤ºä¾‹ä»£ç )

---

## 1ï¸âƒ£ å®‰è£…ä¸è®¾ç½®

### åœ¨ Jupyter Notebook æ–° Cell ä¸­è¿è¡Œ:

```python
# å®‰è£… Hugging Face Hub åº“
!pip install huggingface_hub
```

---

## 2ï¸âƒ£ è·å– API Token

### æ­¥éª¤:
1. è®¿é—® [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. ç‚¹å‡» "New token"
3. é€‰æ‹© "Read" æƒé™å³å¯
4. å¤åˆ¶ç”Ÿæˆçš„ token

### åœ¨ Google Colab ä¸­ä¿å­˜ Token:
1. ç‚¹å‡»å·¦ä¾§ ğŸ”‘ å›¾æ ‡ (Secrets)
2. æ·»åŠ æ–°çš„ Secret:
   - Name: `HuggingFace`
   - Value: ç²˜è´´ä½ çš„ token

---

## 3ï¸âƒ£ åŸºç¡€ä½¿ç”¨

### Cell 1: å¯¼å…¥åº“å¹¶åˆ›å»ºå®¢æˆ·ç«¯

```python
from huggingface_hub import InferenceClient
import os

# æ–¹æ³• A: ä» Colab Secrets è¯»å– (æ¨è)
from google.colab import userdata
hf_token = userdata.get('HuggingFace')

# æ–¹æ³• B: ç›´æ¥è®¾ç½® (ä¸æ¨è,ä¼šæš´éœ² token)
# hf_token = "hf_your_token_here"

# åˆ›å»ºå®¢æˆ·ç«¯
client = InferenceClient(token=hf_token)

print("âœ… Hugging Face å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ!")
```

### Cell 2: ç®€å•æµ‹è¯•

```python
# æµ‹è¯•åŸºç¡€å¯¹è¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„ AI åŠ©æ‰‹,è¯·ç”¨ç¹ä½“ä¸­æ–‡å›ç­”ã€‚"},
    {"role": "user", "content": "ä½ å¥½!è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
]

response = client.chat_completion(
    messages=messages,
    model="Qwen/Qwen2.5-7B-Instruct",
    max_tokens=200
)

print(response.choices[0].message.content)
```

---

## 4ï¸âƒ£ Lucky Vicky é›†æˆ

### Cell 3: åˆ›å»º Hugging Face ç‰ˆæœ¬çš„ Lucky Vicky

```python
def lucky_post_hf(event, client):
    """
    ä½¿ç”¨ Hugging Face çš„å“¡ç‘›å¼æ€è€ƒç”Ÿæˆå™¨
    
    å‚æ•°:
        event: å‘ç”Ÿçš„äº‹ä»¶
        client: HuggingFace InferenceClient
    
    è¿”å›:
        Lucky Vicky é£æ ¼çš„æ­£å‘è´´æ–‡
    """
    system_prompt = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": event}
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",  # ä¸­æ–‡èƒ½åŠ›å¼ºçš„æ¨¡å‹
        max_tokens=500,
        temperature=0.8  # æ›´æœ‰åˆ›æ„
    )
    
    return response.choices[0].message.content

# æµ‹è¯•
event = "ä»Šå¤©å‡ºé–€å¿˜è¨˜å¸¶å‚˜,çµæœä¸‹å¤§é›¨"
result = lucky_post_hf(event, client)
print(f"äº‹ä»¶: {event}\n")
print(f"ğŸ“£ å“¡ç‘›å¼è²¼æ–‡:\n{result}")
```

---

## 5ï¸âƒ£ å®Œæ•´ç¤ºä¾‹ä»£ç 

### Cell 4: å¤šåŠŸèƒ½ç¤ºä¾‹

```python
# ============================================
# ç¤ºä¾‹ 1: æ–‡æœ¬ç”Ÿæˆ
# ============================================
def hf_text_generation(prompt, client):
    response = client.text_generation(
        prompt=prompt,
        model="Qwen/Qwen2.5-1.5B-Instruct",
        max_new_tokens=200
    )
    return response

# æµ‹è¯•
print("ç¤ºä¾‹ 1: æ–‡æœ¬ç”Ÿæˆ")
print(hf_text_generation("å°ç£æœ€æœ‰åçš„å¤œå¸‚æ˜¯", client))
print("\n" + "="*50 + "\n")


# ============================================
# ç¤ºä¾‹ 2: å¯¹è¯è¡¥å…¨
# ============================================
def hf_chat(user_message, client, system_message="ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„åŠ©æ‰‹"):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=300
    )
    
    return response.choices[0].message.content

# æµ‹è¯•
print("ç¤ºä¾‹ 2: å¯¹è¯è¡¥å…¨")
print(hf_chat("è«‹æ¨è–¦ä¸‰å€‹å°åŒ—çš„æ™¯é»", client))
print("\n" + "="*50 + "\n")


# ============================================
# ç¤ºä¾‹ 3: æƒ…æ„Ÿåˆ†æ
# ============================================
def hf_sentiment(text, client):
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯æƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚åˆ†ææ–‡æœ¬æƒ…æ„Ÿ,åªå›ç­”:æ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§ã€‚"
        },
        {
            "role": "user",
            "content": f"åˆ†ææƒ…æ„Ÿ: {text}"
        }
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-1.5B-Instruct",
        max_tokens=20
    )
    
    return response.choices[0].message.content

# æµ‹è¯•
print("ç¤ºä¾‹ 3: æƒ…æ„Ÿåˆ†æ")
print(f"æ–‡æœ¬: é€™å€‹ç”¢å“çœŸçš„å¤ªæ£’äº†!")
print(f"æƒ…æ„Ÿ: {hf_sentiment('é€™å€‹ç”¢å“çœŸçš„å¤ªæ£’äº†!', client)}")
print("\n" + "="*50 + "\n")


# ============================================
# ç¤ºä¾‹ 4: ç¿»è¯‘
# ============================================
def hf_translate(text, target_lang, client):
    messages = [
        {
            "role": "system",
            "content": f"ä½ æ˜¯ä¸“ä¸šç¿»è¯‘ã€‚å°†æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}ã€‚"
        },
        {
            "role": "user",
            "content": text
        }
    ]
    
    response = client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-7B-Instruct",
        max_tokens=200
    )
    
    return response.choices[0].message.content

# æµ‹è¯•
print("ç¤ºä¾‹ 4: ç¿»è¯‘")
original = "Machine learning is transforming the world"
translated = hf_translate(original, "ç¹é«”ä¸­æ–‡", client)
print(f"åŸæ–‡: {original}")
print(f"è¯‘æ–‡: {translated}")
print("\n" + "="*50 + "\n")


# ============================================
# ç¤ºä¾‹ 5: æµå¼å“åº” (é€å­—è¾“å‡º)
# ============================================
def hf_stream(prompt, client):
    messages = [{"role": "user", "content": prompt}]
    
    print(f"æç¤º: {prompt}\nå›å¤: ", end="")
    
    for token in client.chat_completion(
        messages=messages,
        model="Qwen/Qwen2.5-1.5B-Instruct",
        max_tokens=200,
        stream=True
    ):
        chunk = token.choices[0].delta.content
        print(chunk, end="", flush=True)
    
    print("\n")

# æµ‹è¯•
print("ç¤ºä¾‹ 5: æµå¼å“åº”")
hf_stream("è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹å°ç£", client)
```

---

## 6ï¸âƒ£ æ•´åˆåˆ° Gradio App

### Cell 5: æ·»åŠ  Hugging Face é€‰é¡¹åˆ°ç°æœ‰ App

```python
import gradio as gr

# å®šä¹‰ç³»ç»Ÿæç¤º
system = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""

# å¤šæä¾›å•†æ”¯æŒ
def lucky_post_multi(prompt, provider_choice):
    """æ”¯æŒå¤šä¸ª AI æä¾›å•†"""
    
    if provider_choice == "Hugging Face":
        # ä½¿ç”¨ Hugging Face
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]
        response = client.chat_completion(
            messages=messages,
            model="Qwen/Qwen2.5-7B-Instruct",
            max_tokens=500,
            temperature=0.8
        )
        return response.choices[0].message.content
    
    elif provider_choice == "Groq":
        # ä½¿ç”¨åŸæœ‰çš„ Groq
        return reply(system=system, prompt=prompt, 
                    provider="groq", model="openai/gpt-oss-120b")
    
    else:
        return "è¯·é€‰æ‹©ä¸€ä¸ªæä¾›å•†"

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="å“¡ç‘›å¼æ€è€ƒç”¢ç”Ÿå™¨ - å¤šæ¨¡å‹ç‰ˆ") as demo:
    gr.Markdown("### ê’°*ËŠáµ•Ë‹ê’± å“¡ç‘›å¼æ€è€ƒç”¢ç”Ÿå™¨ Lucky Vicky ğŸŒˆ")
    gr.Markdown("æ”¯æŒå¤šä¸ª AI æä¾›å•†!è¯·é€‰æ‹©ä½ æƒ³ä½¿ç”¨çš„æ¨¡å‹")
    
    with gr.Row():
        user_input = gr.Textbox(
            label="ä»Šå¤©ç™¼ç”Ÿçš„äº‹æƒ…æ˜¯â€¦", 
            placeholder="ä¾‹å¦‚:ä»Šå¤©å‡ºé–€å°±ä¸‹å¤§é›¨, å¯æ˜¯å¿˜äº†å¸¶å‚˜...",
            lines=3
        )
    
    # æ¨¡å‹é€‰æ‹©
    provider_dropdown = gr.Dropdown(
        choices=["Groq", "Hugging Face"],
        value="Hugging Face",
        label="ğŸ¤– é¸æ“‡ AI æä¾›å•†"
    )
    
    submit_btn = gr.Button("âœ¨ Lucky Vicky é­”æ³•!", variant="primary")
    output = gr.Textbox(label="ğŸ“£ å“¡ç‘›å¼è²¼æ–‡", lines=10)
    
    # æ·»åŠ ç¤ºä¾‹
    gr.Examples(
        examples=[
            ["ä»Šå¤©å’–å•¡ç‘åˆ°é›»è…¦ä¸Šäº†!", "Hugging Face"],
            ["å‡ºé–€å¿˜è¨˜å¸¶å‚˜,çµæœä¸‹å¤§é›¨", "Hugging Face"],
            ["è€ƒè©¦è€ƒå¾—ä¸å¤ªå¥½", "Groq"],
        ],
        inputs=[user_input, provider_dropdown]
    )
    
    submit_btn.click(
        fn=lucky_post_multi, 
        inputs=[user_input, provider_dropdown], 
        outputs=output
    )

demo.launch(share=True, debug=True)
```

---

## ğŸ“Š æ¨èçš„ Hugging Face æ¨¡å‹

| æ¨¡å‹åç§° | å¤§å° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| `Qwen/Qwen2.5-1.5B-Instruct` | 1.5B | è½»é‡å¿«é€Ÿ | ç®€å•å¯¹è¯ã€æµ‹è¯• |
| `Qwen/Qwen2.5-7B-Instruct` | 7B | ä¸­æ–‡èƒ½åŠ›å¼º | Lucky Vickyã€ç¿»è¯‘ |
| `Qwen/Qwen2.5-72B-Instruct` | 72B | æœ€å¼ºæ€§èƒ½ | å¤æ‚ä»»åŠ¡ |
| `meta-llama/Llama-3.1-8B-Instruct` | 8B | å¤šè¯­è¨€ | é€šç”¨å¯¹è¯ |
| `google/gemma-2-9b-it` | 9B | Google å‡ºå“ | å¹³è¡¡æ€§èƒ½ |

---

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. Temperature å‚æ•°
- `0.1-0.3`: æ›´ç¡®å®šã€ä¸€è‡´çš„å›å¤
- `0.7-0.9`: æ›´æœ‰åˆ›æ„ã€å¤šæ ·çš„å›å¤
- `1.0+`: éå¸¸éšæœº

### 2. Max Tokens
- ç®€çŸ­å›å¤: 50-100
- ä¸­ç­‰å›å¤: 200-300
- é•¿æ–‡æœ¬: 500-1000

### 3. é”™è¯¯å¤„ç†
```python
try:
    response = client.chat_completion(...)
except Exception as e:
    print(f"é”™è¯¯: {e}")
    # å¯ä»¥åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹
```

---

## ğŸ”— ç›¸å…³é“¾æ¥

- [Hugging Face Hub](https://huggingface.co/)
- [æ¨¡å‹åº“](https://huggingface.co/models)
- [API æ–‡æ¡£](https://huggingface.co/docs/huggingface_hub)
- [è·å– Token](https://huggingface.co/settings/tokens)

---

## âœ… å®Œæˆ!

ç°åœ¨æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨ Hugging Face Inference API!

**ä¸‹ä¸€æ­¥:**
1. è·å–æ‚¨çš„ API Token
2. åœ¨ Colab Secrets ä¸­ä¿å­˜
3. è¿è¡Œä¸Šé¢çš„ç¤ºä¾‹ä»£ç 
4. å°è¯•ä¸åŒçš„æ¨¡å‹å’Œå‚æ•°

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«! ğŸ‰
