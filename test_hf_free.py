"""
Hugging Face æµ‹è¯• - ä½¿ç”¨å…è´¹å¯ç”¨çš„æ¨¡å‹
"""

from huggingface_hub import InferenceClient
import os

# æ‚¨çš„ Token (è¯·ä»ç¯å¢ƒå˜é‡æˆ– .env æ–‡ä»¶è¯»å–)
TOKEN = os.getenv('HF_TOKEN', 'your_huggingface_token_here')

print("="*60)
print("ğŸ¤— Hugging Face API æµ‹è¯•")
print("="*60)

try:
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = InferenceClient(token=TOKEN)
    print("\nâœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    
    # æµ‹è¯• 1: ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆ API (ä¸æŒ‡å®šç‰¹å®šæ¨¡å‹)
    print("\n" + "="*60)
    print("æµ‹è¯• 1: åŸºç¡€æ–‡æœ¬ç”Ÿæˆ")
    print("="*60)
    
    try:
        response = client.text_generation(
            "è«‹ç”¨ç¹é«”ä¸­æ–‡èªª:ä½ å¥½",
            max_new_tokens=50
        )
        print(f"âœ… æˆåŠŸ!")
        print(f"å›å¤: {response}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    # æµ‹è¯• 2: ä½¿ç”¨å¯¹è¯ API
    print("\n" + "="*60)
    print("æµ‹è¯• 2: å¯¹è¯ API")
    print("="*60)
    
    try:
        messages = [
            {"role": "user", "content": "è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹å°ç£"}
        ]
        
        # å°è¯•ä½¿ç”¨ meta-llama æ¨¡å‹ (é€šå¸¸å…è´¹å¯ç”¨)
        response = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-1B-Instruct",
            max_tokens=100
        )
        print(f"âœ… æˆåŠŸ!")
        print(f"å›å¤: {response.choices[0].message.content}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    # æµ‹è¯• 3: Lucky Vicky (ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹)
    print("\n" + "="*60)
    print("æµ‹è¯• 3: Lucky Vicky ç”Ÿæˆå™¨")
    print("="*60)
    
    try:
        system = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å¯«é€™æ®µ po æ–‡:
è«‹ç”¨å“¡ç‘›å¼æ€è€ƒ, ä¹Ÿå°±æ˜¯ä»€éº¼éƒ½æ­£å‘æ€ç¶­ä»»ä½•ä½¿ç”¨è€…å¯«çš„äº‹æƒ…,
ç”¨æˆ‘çš„ç¬¬ä¸€äººç¨±ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡,
èªªç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹, ä¸¦ä¸”ä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚
å¯ä»¥é©åº¦çš„åŠ ä¸Š emojiã€‚"""
        
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": "ä»Šå¤©å’–å•¡ç‘äº†"}
        ]
        
        response = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-3B-Instruct",
            max_tokens=300,
            temperature=0.8
        )
        
        result = response.choices[0].message.content
        print(f"âœ… æˆåŠŸ!")
        print(f"\nğŸ“£ Lucky Vicky è²¼æ–‡:\n{result}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆ!")
    print("="*60)
    
    print("""
ğŸ’¡ è¯´æ˜:
- Hugging Face Inference API å¯¹å…è´¹ç”¨æˆ·æœ‰æ¨¡å‹é™åˆ¶
- Qwen æ¨¡å‹å¯èƒ½éœ€è¦ä»˜è´¹è®¢é˜…
- Meta Llama æ¨¡å‹é€šå¸¸å…è´¹å¯ç”¨
- å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥,å»ºè®®ä½¿ç”¨ Groq API (æ‚¨å·²é…ç½®)
    """)
    
except Exception as e:
    print(f"\nâŒ æ€»ä½“é”™è¯¯: {e}")
    print("""
ğŸ’¡ å»ºè®®:
1. æ£€æŸ¥ Token æ˜¯å¦æœ‰æ•ˆ: https://huggingface.co/settings/tokens
2. ç¡®è®¤è´¦æˆ·çŠ¶æ€
3. æˆ–ä½¿ç”¨ Groq API (è¿è¡Œ lucky_vicky_groq.py)
    """)

input("\næŒ‰ Enter é”®é€€å‡º...")
